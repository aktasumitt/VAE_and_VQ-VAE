# VAE and VQ-VAE

## Introduction:
In this project, I aimed to train a Vector Quantized Variational Autoencoder (VQ-VAE) model with Cifar10 dataset and Variational Autoencoder (VAE) model with Mnist dataset

## Dataset:
- I used the Cifar10 and Mnist datasets for this project, which consists of 10 labels with total 60k images on train and 10k images on test.
- size of images are (32x32x3) in Cifar and (28x28x3) in Mnist

## Model:
- To understand the VQ-VAE and VAE model, we first need to understand Autoencoders (AE) model.

#### Autoencoders (AE) :
-  Autoencoder models have the capability to compress an object into smaller dimensions and then reproduce it.
-  It is divided into two parts as Encoder and Decoder. 
-  The Encoder compresses the image dimensions, reducing them to a smaller representation (bottleneck).
-  The Decoder then tries to reconstruct the original image by decoding this representation.
-  However, variations such as dimension reduction or color changes can also be used.

#### Variational Autoencoders (VAE) :
- VAE is essentially a slightly more advanced version of an autoencoder, where the representation consists of variance and mean values.
-  This will apply a more accurate compression and the representations can be learned better with these.
-  However, variance and mean are not learnable parameters, so the variance is multiplied by a parameter called epsilon and added to the mean, transforming these values into learnable parameters.
-  Afterwards, the generated representation is decoded to obtain results very close to the input.

#### Vector Quatized Variational Autoencoders (VQ-VAE):
- VQ-VAE, on the other hand, is an advanced version of VAE where instead of variance and mean, a learnable embedding vector is used.
- The distance between the parameters of the representation generated by the encoder and this vector is calculated, and the parameters of this representation are replaced with the vector parameter with the minimum distance, a process called vector quantization.
- It has been discovered that this process allows the model to converge to accurate results faster.
- Then, the decoder tries to reconstruct the input images by decoding the compressed data. 
- These Autoencoders are also the basis of advanced models such as VQ-GANs and DALL-E.

## Train:
- Based on the explanations above, we have created VQ-VAE and VAE models.
##### VAE Training:
-  The VAE model uses Adam optimizer and two types of loss: Binary Cross-Entropy (BCE) loss and KL-divergence loss. 
-  KL Divergence is a metric that measures how different two probability distributions are from each other.
-   For calculation, the KL Divergence value is obtained between the distribution formed by the mean and variance values of the encoder output and the Normal Distribution N(0,1). This value, along with the BCE loss sum, gives us the total loss value. The model is trained by attempting to minimize this loss.

##### VQ-VAE Training:
- The VQ-VAE model is created with Adam optimizer as the optimizer and two types of loss: Quantize loss and Reconstructor loss.
- The Reconstructor loss is Mean Squared Error (MSE) loss, while Quantize loss helps to better learn the latent space, i.e., the embedding in the model.This will lead us to better representations. 
- The sum of these two losses gives us the total loss.
-  The model is trained to minimize these losses.

## Results and Usage:
- At the end of 20 epochs, images very close to real images were obtained.
- Since they are not very comprehensive models, or rather, because they provide ideas and foundations for larger models, instead of deploying them separately with larger models, I have uploaded them as two Jupyter files.
-  You can directly run them all to train and use them. The training will take a short time.
-  They will save in relative directory
