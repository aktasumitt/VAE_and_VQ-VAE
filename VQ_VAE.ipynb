{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quatized Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torchvision import datasets,transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Control Device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices=(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs=25\n",
    "Learning_rate=2e-4\n",
    "Num_Embeddings=128\n",
    "Channel_size=3\n",
    "save_path=\"VQ-VAE_model.pth.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create Transforms, Dataset and Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "transformer=transforms.Compose([transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# Train Dataset\n",
    "train_dataset=datasets.CIFAR10(\"Cifar10\",train=True,transform=transformer,download=True)\n",
    "\n",
    "# Train Dataloader\n",
    "train_dataloader=DataLoader(dataset=train_dataset,batch_size=64,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    VQ-VAE Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ_VAE(nn.Module):\n",
    "    def __init__(self,channel_size,num_embeddings):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.initial_layer,self.res_block1,self.res_block2=self.Encoder(channel_size=channel_size)\n",
    "        self.res_block1_d,self.res_block2_d,self.out_layer=self.Decoder(channel_size=channel_size)\n",
    "        self.embedding=nn.Embedding(num_embeddings=num_embeddings,embedding_dim=256)\n",
    "    \n",
    "    # Encoder Blocks:\n",
    "    def Encoder(self,channel_size):\n",
    "        initial_layer=nn.Sequential(nn.Conv2d(in_channels=channel_size,out_channels=256,kernel_size=4,padding=1,stride=2,padding_mode=\"reflect\"),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=256,out_channels=256,stride=2,kernel_size=4,padding=1,padding_mode=\"reflect\"),\n",
    "                                    nn.ReLU())\n",
    "        res_block1=nn.Sequential(nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1,padding_mode=\"reflect\"),\n",
    "                                 nn.ReLU())\n",
    "        res_block2=nn.Sequential(nn.Conv2d(in_channels=256,out_channels=256,kernel_size=1))\n",
    "        return initial_layer,res_block1,res_block2\n",
    "    \n",
    "    # Decoder Blocks:\n",
    "    def Decoder(self,channel_size):\n",
    "\n",
    "        res_block1=nn.Sequential(nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=3,padding=1),\n",
    "                                 nn.ReLU())\n",
    "        res_block2=nn.Sequential(nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=1))\n",
    "        \n",
    "        out_layer=nn.Sequential(nn.ConvTranspose2d(in_channels=256,out_channels=256,kernel_size=4,stride=2,padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.ConvTranspose2d(in_channels=256,out_channels=channel_size,stride=2,kernel_size=4,padding=1),\n",
    "                                nn.ReLU())\n",
    "        \n",
    "        return res_block1,res_block2,out_layer\n",
    "    \n",
    "    # Latent Space ,calculate distances between embeddings and encoder output:\n",
    "    def latent_space(self,pre_quatized,embedding):\n",
    "        \n",
    "        # Permute data because last dimesion should be channel\n",
    "        data_perm=pre_quatized.permute(0,3,2,1)\n",
    "        \n",
    "        # Reshape (batch_size,last_img_size*last_img_size,channel)\n",
    "        quanted_in=data_perm.reshape(data_perm.shape[0],-1,data_perm.shape[-1])\n",
    "        \n",
    "        # Claculate distance\n",
    "        dist=torch.cdist(quanted_in,embedding.weight[None,:].repeat(quanted_in.shape[0],1,1))\n",
    "        \n",
    "        # Min distance index\n",
    "        min_distance_indexes=torch.argmin(dist,dim=-1)\n",
    "        \n",
    "        # Change\n",
    "        quanted_out=torch.index_select(embedding.weight,0,min_distance_indexes.view(-1))\n",
    "        \n",
    "        return quanted_in,quanted_out\n",
    "\n",
    "    \n",
    "    def forward(self,data):\n",
    "        \n",
    "        # Encoder Block\n",
    "        x=self.initial_layer(data)\n",
    "        x_r=self.res_block1(x)\n",
    "        \n",
    "        x_r=x_r+x   # Residual block\n",
    "    \n",
    "        pre_quatized=self.res_block2(x_r)\n",
    "        \n",
    "        pre_quatized=pre_quatized+x_r    # Residual block\n",
    "        \n",
    "        \n",
    "        # Quatized vector with embeddings vector\n",
    "        quanted_in,quanted_out=self.latent_space(pre_quatized=pre_quatized,embedding=self.embedding)\n",
    "        quanted_in=quanted_in.reshape((-1,quanted_in.size(-1)))\n",
    "        \n",
    "        # Calculate Quantize Loss\n",
    "        loss1=torch.mean((quanted_out.detach()-quanted_in)**2) \n",
    "        loss2=torch.mean((quanted_out-quanted_in.detach())**2)  \n",
    "        loss_quantize=loss1+(0.25*loss2)                        \n",
    "        \n",
    "        quanted_out=quanted_in+(quanted_out-quanted_in).detach()\n",
    "        \n",
    "        # Reshape for decoder input (batch_size,channel,last_img_size,last_img_isze)\n",
    "        B,C,H,W=pre_quatized.shape \n",
    "        quanted_out_reshaped=quanted_out.reshape(B,H,W,C).permute(0,3,1,2)  # reshape to be decoder input that its shape Ä°S like the encoder output\n",
    "        \n",
    "        # Decoder \n",
    "        d_out1=self.res_block1_d(quanted_out_reshaped)        \n",
    "        \n",
    "        d_out2=self.res_block2_d(d_out1)\n",
    "        \n",
    "        d_out=d_out1+d_out2 # Residual\n",
    "        \n",
    "        out=self.out_layer(d_out)\n",
    "        \n",
    "        return out,loss_quantize    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model=VQ_VAE(channel_size=Channel_size,num_embeddings=Num_Embeddings)\n",
    "model.to(devices)\n",
    "\n",
    "# Optimizer\n",
    "optimizer=torch.optim.Adam(params=model.parameters(),lr=Learning_rate)\n",
    "\n",
    "# Reconsturacter Loss\n",
    "loss_fn=nn.MSELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Model save:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_model(model,optimizer,epoch,path):\n",
    "    checkpoints={\"Epochs\":epoch,\n",
    "                 \"Optimizer_state\":optimizer.state_dict,\n",
    "                 \"Model_states\": model.state_dict}\n",
    "    print(\"Model is saving...\")\n",
    "    torch.save(checkpoints,f=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(epochs,train_dataloader,optimizer,model,loss_fn,Save_path,save_model_fn):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        prograss_bar=tqdm.tqdm(range(len(train_dataloader)),\"Train proccess\")\n",
    "        \n",
    "        for batch,(img,_label_) in enumerate(train_dataloader):\n",
    "            \n",
    "            img=img.to(devices)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out,loss_quantize=model(img)\n",
    "            reconstruction_loss=loss_fn(out,img)\n",
    "            loss = reconstruction_loss+loss_quantize\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            prograss_bar.update(1)   \n",
    "        \n",
    "        prograss_bar.set_postfix({\"EPOCH\":epoch+1,\"step\":batch+1,\"LOSS\": (loss.item()/(batch+1))})\n",
    "        prograss_bar.close()\n",
    "        save_model_fn(model,optimizer,epoch+1,Save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training(epochs=Epochs,train_dataloader=train_dataloader,optimizer=optimizer,model=model,loss_fn=loss_fn,Save_path=save_path,save_model_fn=Save_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Visualize Generated Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize_Test(Test_dataloader,model):\n",
    "    with torch.no_grad():\n",
    "        for batch,(img_test,_) in enumerate(Test_dataloader):\n",
    "            \n",
    "            img_test=img_test.to(devices)\n",
    "            \n",
    "            out_img,_=model(img_test)\n",
    "            \n",
    "            if batch==0:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    for i in range(20):\n",
    "        plt.subplot(4,5,i+1)\n",
    "        out_img=out_img.cpu()\n",
    "        # print(out_img[i].shape)\n",
    "    \n",
    "        plt.imshow(torch.permute(out_img[i],(1,2,0)))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize_Test(Test_dataloader=train_dataloader,model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
